spring:
  profiles:
    active: dev,test

logging:
  level:
    com:
      org:
        journalApp: DEBUG


#video-26
#topic-Spring Boot Profiles

#generally server and ports are same and can be written here in the general yml okay,
#but the mongo db server, username & password and other configurations can be different
#so for these profile specific configuration we write them in different-different  yml files respectively
#but now just for example we have created multiple ports for different profile as we do not have different properties to configure here

#so all dev related configuration will be in dev yml similar for the prod & test or other profiles if we have
#I have run the application for all profile locally, everything working fine.
#now if we have only dev and prod yml only and then run, then it will not work as it will get confuse which one to pick if not mentioned
#so for that we can configure the run configuration environment variable like-->spring.profiles.active=dev

#now when we set up like this then it will work in intellij but, on production it will not work
#now we have run the commands in terminal to create jar using the dev profile using the below commands
#--> mvn clean package --> -D spring.profiles.active=dev --> cd target --> java -jar jar-name-snapshot.jar --spring.profiles.active=dev
# above -D flag is used to det jvm properties and in the second way we are not setting the jvm property

#As we can not access production server from our local machine so setting the env var to prod does not make any sense
#cause companies do white listing of the production server so that only authorized one can access it
#so that is why we pass the profile with jar on prod as there we can't use the env variables

#now for the production we were using terminal and writing commands so to ease these work we got
#jenkins which has GUI to do all these things to set profiles create package all these related commands
#jenkins can be set up on the dev and prod environment depends on the company

#now based on profile we can set that we want to load this bean or not using @Profile annotation on it
#okay so one more annotation is @Active Profile we can use this in test classes over a class
#okay so in the main method we can use get environment and get profile methods to know the active profile
#okay so now the one imp thing is that we can use coma separated active profiles to use multiple profiles at the same time in the yml

#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-27
#topic-Logging In Spring Boot

#Introduction--
#let say your application is deployed on production server and some API is showing unexpecting behaviour while in local it is working fine
#so if we have done logging in our spring boot application, then in that case we can track the production issues using logging
#okay so we will run the API and track the logs for the issues, using logging we can troubleshoot and monitor our application
#three major frameworks are there in spring boot which are used for logging which are--> logback --> log4j2 --> java util logging

#Logging-Frameworks--
#logback--> default logging frameworks used in spring boot application used for flexible configuration and good performance
#log4j2--> widely used framework have features like asynchronous logging and support various output formats
#javaUtilLogging--> included in the JSE part of java development with basic logging feature which are less efficient than other third party logging frameworks, part of jdk

#some-important-info--
#default logback configuration is embedded in the spring boot libraries and not visible in project's source code
#to customize the logging configuration we write logback.xml in the resource directory, so when our spring detects
#this then it uses this file instead of the default logging
#we can configure logging in spring-boot using the properties and the yaml files and also the logback.xml

#logging-levels--
#logging levels helps us to categorize the log statement based on the saviour-ity of the log, common logging levels are-->
#Trace --> Debug --> Info --> Warn --> Error
#the default logging is enabled for info --> warn --> error

#we use @Slf4j and @Log4j annotation in spring boot to initialize logger instances to our class
#we use logger to print in files as we cant use the print

#steps to implement logging in spring boot--
#at first, we create the logger instance in that class, each class has its own private static final logger instance like
#LoggerFactory.getLogger(this_class.class);make sure to import slf4j.Logger & LoggerFactory
#okay so after the instance and all now in case of any failure, let say in catch we can call this all levels which are methods
#using the logger instance, always remember the class in which we are using the logger is the same class we have to pass in the logger
#in the logger methods we can pass placeholders and pass arguments for it, and also we can pass the exception object too.

#some-important-info-Slf4j--
#if we use @Slf4j annotation on this class which is a lombok annotation then it
#injects the logger instance to the class so we can skip manually creating the instance as given below
#but the instance created using the annotation is log not logger, so use log instead of logger provided by lombok
#private static final Logger logger= LoggerFactory.getLogger(UserService.class);

#Logging Using YAML file--
#now for the remaining two levels which are not default configured by the slf4j we have configured them in the application.yml u can check above
#okay so if I configure error for example in the yml then error and logging levels having more saviour than error all are accessible through the log
#Most important--> the logs will be printed like in our case we have written in catch, so after the exception only it will print the log otherwise not
#--------------------------
#now if we don't want to print error ones then we can write--> logging:level:root:ERROR , so only error logs get printed
#now if we don't want to print any log then we can write--> logging:level:root:OFF , no logs get printed, similarly we can turn off logs for a class too
#now if we want to stop a particular package logs then in that case we can write package name in place of root in the above example
#now all these above things that we did in the yml can bve done in the logback.xml in order for clarity as these files handles all the logs

#Logging Using logback.xml file--
#the root tag in logback.xml is configuration inside which we write appender and logger configuration enclosed
#Appender--> now where do we want to print the logs this work is taken care by appender e.g.--> Console Appender --> File Appender
#appender is nothing but an way to show the output log, inside appender we write file , encoder & pattern enclosed
#for more u can check the logback.xml everything is there, for each minute log we can refer to video as i have done using appender

#--------------------------------------------------------------------------------------------------------------------------------------------------

#video-28
#topic-SonarQube/SonarCloud/SonarLint

#Introduction--
#In order to check the code standard, bugs, quality, scalability of the code
#we use SonarQube which is a container to which we provide our code
#sonarQube can be deployed on server as similar as mongo db and also on local
#Or on the sonarCloud server also for that we need to put our code on GitHub

#steps to implement sonarQube--
#Download & Extract then go to bean and start sonar by entering the credentials as admin-admin
#before this check the compatibility of it with our jdk and if needed to set java home variable
#set that too and then go ahead with the steps mentioned okay then
#add sonar Dependency compatible with the jdk and sonar version downloaded
#then in the terminal run mvn clean install sonar:sonar or
#mvn clean verify sonar:sonar "-Dsonar.login=token here"  "-Dsonar.host.url=http://localhost:9000" -DskipTests

#SonarLint--
#generally people use sonarLint plugin instead of sonarQube
#and now they have renamed or combined the sonarlint and sonarQube combined as sonarQube plugin
#now after installing plugin it starts to show the same suggestion in our code as we used to see in the Qube

#SonarCloud--
#it is same like local but this is on cloud we are going to fetch all the app activity on sonar cloud
#and we can fetch this using github actions okay for which some steps are given adding dependency to pom
#then creating build.yml in .github/workflows/build.yml like these okay then with each push to GitHub
#our sonar cloud is also going to pick up the changes and check it standard and code quality of the new commit

#----------------------------------------------------------------------------------------------------------------------------------------------------

#video-29 & 30
#topic-External API Integration

#Introduction--
#how we can hit the get and post API through code not through postman okay and how we can get the response using code

#Steps to implement external API--

#GET API--
#first if sign in and get the free or paid API key whatever u want
#then create one package as api.response and create one class API_Name_Response using this class we are going to de-serialization from json response to pojo/java object
#then we define getter setter for the response class, and also json property annotation to tell how that variable is declared in json response
#then we create RestTemplateConfig to return the Rest Template object, okay then we create API_Name_Service in that we write the API key provided to us
#but not hard code it, we define it in yml like api_name:api:key: api_token_here okay and use @Value("${api_name.api.key=api_token}") on the api key variable to initialize it
#then we create one other variable which is the website API inside which we pass our API key and the Query and then we autowired the rest template here

#then we create a method and pass the query parameter in it then here we finalize the web_Api by replacing the API_key and Query parameter using .replace() method
#then we get the response using restTemplate.exchange(final_API, HttpMethod.type, requestEntityForAuth, ResponsePOJO.class);
#on the response we use .getBody() to get the response and also we can return this response so it can be called using the controller
#also on the response we can get the http status code and use it however we want got it

#now the final steps as we are getting the response so we will create a GetMapping
#and if the user is authenticated then we get his username as we used to do
#then we call the service and the method we created and pass the argument in it
#and if our response is nested POJO then on that we can call nested methods to get the response
#and return the response entity as we always do in the controller here is how we get the external API response

#POST API--
#now the Post call we are making is from postman and we are working in our microservice which the Journal App itself okay so in future it can happen that
#we create another projects/microservices and have deployed the jar on some server now using the IP and port of that we can send the request
#then we can consume those post api in the similar way like with our request we can send the data to other microservices
#everything is same as above just in place of the get call we do post and the request entity is not empty we pass request body in place of that
#here is how we do it String requestBody= "{---}" then we pass it in the HttpEntity okay then we pass this httpEntity reference to the restTemplate.exchange() here
#and also along with these request body we can pass the headers using the httpHeaders.set("key","value"); too in the httpEntity as we do for our authenticated API for authorization

#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-31 & 32 & 33
#topic-Eleven Labs API / Service Vs Component / @Value Annotation

#Eleven Labs--
#taught how to use third party API using the documentation, how we can get all the parameters,
#headers, id's and everything and then pass the cURL and get the response , curl is a cmd tool
#we can paste curl in postman it will auto configure the request and everything and url, we can also
#save response to file in postman if any if we want to use the http request in command line then we use curl
#for more u can refer to the video itself video 31 of the spring boot playlist

#Service Vs Component--
#both the annotation are used to create bean okay but to differentiate that these class contains business logic
#we use @Service annotation instead of @Component okay, these increase readability so that's why we use it

#@ValueAnnotation--
#we define it in yml like api_name:api:key: api_token_here okay and use @Value("${api_name.api.key=api_token}") on the api key variable to initialize it
#okay please take care if we declare the api_key inside our yml, then we shouldn't push our yml to GitHub, we have test.yml for that keep variable there as we have not pushed it on github

#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-34
#topic-@PostConstruct

#Introduction--
#@PostConstruct annotation is applied on method and when that class's bean created at the same time the post construct method get invoked
#okay so for the frequently used data we create a configuration_collection in our database or write them in yml okay, we treat that collection as a configuration
#so we can configure all our API keys and values in the database and fetch from there but if we fetch it each time from the database then its not good practice so
#instead of this we use application cache where we load all this keys and values and use them accordingly

#Application Cache--
#is a way in which we put frequently used and frequently changing data or configurations in database and load them once in our spring boot application
#to implement this we create a cache package inside that an AppCache class okay and one map as appCache
#bro this all video is too complex u can refer to classes AppCache JournalAppConfiguration related classes and WeatherService greetings message and the Placeholders class
#or after completing this project we will conclude this one video with the remaining concepts that are left to be noted

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-35
#topic-MongoTemplate/Criteria/Query

#so what happens in repository is that during run time spring boot injects the implementation of the repository where ever they are used
#and the second thing these repository use Query method DSL which means we write the method in such a way that it becomes the query,
#this is query of the method name is made by spring boot at runtime

#Drawaback of query method DSL--
#We should always know the naming convention otherwise it will not form query okay
#and also when we write method name we do not get the suggestion, as well it has other limitations too
#we cant write the complex query using DSL method cause we cant use the Dot operator to know the methods
#so to write the complex queries we use criteria instead of DSL

#Criteria Method--
#Criteria and Query goes hand in hand both work together
#in the method we create one Query Object (import Mongo query) then use addCriteria() method on reference and inside that we use
#Criteria.anyMethod.againMethod like that we do, now to use this Criteria and Query we use MongoTemplate which is a class,
#Spring:Data:MongoDB this provides us the MongoTemplate class automatically, so we do not need to configure its bean explicitly
#so we use mongo template to interact with the database, as it has its multiple methods to work with MongoDB
#so all the methods of MongoTemplate need two parameters one is query and other is the entity class where collection is mapped
#listen whenever we use @Data annotation on entity then in that case we have to write the constructor explicitly if we need
#okay so on the where we can use this method too ne,lt,lte,gt,gte
#we can create the instance of the Criteria as criteria okay so that we can access more methods which we cant directly using the class
#okay so all the examples and good practices of the mongo criteria and query and rest template are mentioned in the UserRepositoryImpl

#A good message at the end of this video which is that we can't master this all, we are tend to forget this in a month
#this all will happen with project requirement and experience, so focus on the core things which is DSA and System Design,
#after the functional knowledge of the language and framework okay so follow this and grow ahead with this mindset

#---------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-36
#topic-Spring-Boot-Email

#how we do send email using person email that we are going to implement here
#Step-1 we add mail starter dependency to pom Step2- is to Autowired the JavaMailSender and
#to inject the bean at run time into this we need to configure the email in the yml
#okay so we have configured this in our test yml which we dont push okay has active profile during run so
#this is what we need to paste in the yml to inject the JavaMailSender reference and create its bean
#  mail:
#    host: smtp.gmail.com  -->here it can be smtp.yahoo.com like that u can enter other too
#    port: 587
#    username: patilrishi410@gmail.com
#    password: app password goes here -->u can generate this from your Google security settings
#    properties:
#      mail:
#        smtp:
#          auth: true
#          starttls:      -->this enable the transport layer security which means that all the details will be encrypted during transaction between the app and our email, and we have enabled the encryption that's why port is 587 otherwise it should be 25
#            enable: true

#now we got the bean injected then we will create a method send email with three params which is to, subject and body
#then we create SimpleMailMessage object then on this reference we can use setTo, setSubject, setText for body and pass the SimpleMailMessage
#reference in the javaMailSender.send(mail); like this we can send mail and one more we do this all in try catch, okay so the typ of mail here
#will be JavaMailSender okay, we can use more methods on the mail reference to set the bc and all, and we have successfully tested this, it worked
#for more details u can refer to the EmailService class that we have created

#------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-37 & 38
#topic-Cron Job In Spring Boot/ Adding Sentiment

#CronScheduling--
#to automate things and jobs we use Cron job and Scheduler in Spring Boot So we create a scheduler package and User Scheduler in it
#okay inside that we create a scheduler method and write our all scheduling logic that for which user and for what time and interval we
#want to schedule after doing this all at last we use @Scheduled annotation and pass cron expression in it, we can generate cron expression from cron maker
#okay so tell our spring boot application that in we have scheduled some methods then we use @EnableScheduling on the Main class, then everything works smooth
#for more u can refer to UserScheduler class and SentimentAnalysisServices in which we are going to put our logic soon which is related to machine learning

#Adding Sentiment Logic--
#added the complete sentiment logic in the user scheduler and sent mail okay created sentiment enum, and test u can refer to all

#--------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-39 & 40
#topic-Redis on Wsl & Redis on Cloud

#Introduction--
#Redis is a in memory cache means that it stores the data in the ram, cause ram is fast then hard disk,
#ram because it takes nano seconds to access the data in ram, while disk access take milliseconds
#we use redis for caching

#redis integration to spring boot application--
#

#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-41
#topic-Kafka Masterclass

#kafka is open source distributed event streaming platform
#distributed means kafka can run on multiple servers to handle large amount of data,
#multiple machines will be connected, event means data, so data streaming platform,
#kafka is used tp process the constantly generating data and do constant processing without delays
#for example let say there is a social media app where people are continuously doing
#like comment and all the stuff, these all action are event,constantly generating events,
#kafka will collect all these events, and store them temporarily and then distribute them to multiple services,
#if they want to analyze, can give it to notification service

#kafka ensures flow of data from source to destination smoothly and quickly
#kafka ensures direct communication with services whoever needs the data
#direct api calls are synchronous we achieve asynchronous calls using kafka
#like we put kafka between our two service to get and send the response
#okay now for example let say 1 million user are liking photo then that number of calls
#are directly going to the other service, which will be an heavy load on that service
#after huge load services can be down, so that's why here we put kafka in between which takes the
#one million data and breaks it and distributes it to multiple server and consumed parallel
#okay so that's how kafka achieves tolerance , scalability

#Internal working of kafka and setup--

#kafka cluster-- kafka cluster is group of kafka brokers, kafka broker is a server on which our
#kafka is running, so basically kafka cluster are the group multiple server where kafka is running

#kafka producer-- writes new data into kafka cluster and these data given by kafka producer to kafka cluster is consumed by kafka consumer,
#okay we need to write code in kafka producer, we write an action for the data transfer to cluster form producer

#zookeeper-- zookeeper keeps track of kafka cluster health
#kafka connect-- if we want to bring any entity data to the kafka cluster then we use kafka connect for that,
#it brings dat from any file, database or from anywhere without writing any code
#which is called as declarative integration, okay we declare the things here that we want to get the data from one
#server to other, okay so we sent the data using source to the cluster from connect to cluster while to retrieve data we use sink
#kafka stream-- these are some functionalities which we use for dat transformation, like we will pick some data
#from kafka cluster, and we will do some transformation to it then send back the data to kafka cluster

#Download and Install kafka and zookeeper--

# Kafka WSL Command Notes

#Navigate to Kafka folder
#cd /mnt/d/Imp_Downloads/kafka_2.13-3.9.1

#Start ZooKeeper (must be first)
#bin/zookeeper-server-start.sh config/zookeeper.properties

#Start Kafka Broker (after ZooKeeper is running)
#bin/kafka-server-start.sh config/server.properties

#Create a topic named 'my-topic' with 3 partitions
#bin/kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3

#List all existing Kafka topics
#bin/kafka-topics.sh --list --bootstrap-server localhost:9092

#Describe topic details (partitions, replicas, leader, etc.)
#bin/kafka-topics.sh --describe --topic my-topic --bootstrap-server localhost:9092

#Start Kafka producer (send messages to topic)
#bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic

#Start Kafka consumer (read messages from beginning)
#bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning

# Clear the terminal screen
#clear  # or press Ctrl + L

#kafka topic and partitions--

#when the kafka producer sends the data to broker then it sends the dat in the form of topics,
#topic is similar to table which means similar type of data is in topic, topic live inside broker,
#okay topic inside have partitions, producer can send data in two ways in topic and partitions but
#ultimately it goes in partitions, consumer continuously calls the cluster for the topics to consume data

#partition--a topic is partitioned and distributed to kafka brokers in round-robin fashion to achieve distributed systems
#replication factor--a partition is replicated by this factor in another broker to prevent fault tolerance
#partition is where the actual message is located the final data is located in partition
#whenever we create a topic we need to specify the number of partition, the number can be changed later,
#each particle is ordered, immutable sequence of records each message get stored into partition with an
#incremental id/ off set value, ordering is there only at partition level,
#means if we want the data to be in order then it must have to be in the same single partition
#partitons continuously grows as new records are produced, all the records exist in the distributed log file
#if we send the data along with key then it goes in same partition, it uses hashed key for this
#otherwise without key it goes in round robin, so we can say key is optional, okay as consumer pulls the data from all partition at the same time
#while demonstration it has used "key.separator=-" and "parse.key=true" along with the producer
#cmd okay and for consumer same key separator and --property "print.key=false"

#consumer offset and consumer groups--

#consumer offset--
#position of a consumer in a specific partition of a topic, means which message it is reading,
#offset represents the latest message read by consumer
#when we create a consumer on kafka then a grp id is created which refers to the group to which it belongs
#whgen the consumer group reads the message okay which are in partitions, then each consumer
#keep the track of the offset on which message it is now, as bookmarks
#all these bookmarks are stored in consumer offset which is a built-in topic in apache kafka which keeps track
#of the latest offset commited by consumer group for each partition, okay these topic is internal to kafka
#which cant be read and write directly by clients, these offset info is updated by kafka broker for the clients
#okay so kafka maintain the consumer offset that data is not lost or duplicated--> __consumer_offset
#there are separate consumer offset for each group

#how single consumer reads--reads the topic in round-robin fashion, one consumer is single threaded
#two consumer reads--okay the consumer coordinator divides the topic two both to read the data
#to join group consumer sends request to the grp coordinator then the grp coordinator decides
#which partition to be assigned based on current distribution
#then after this grp coordinator assigns partitions to be consumed, okay partition are assigned in sticky fashion

#we can use commands to list the topic okay use it, similar for consumer group
#we can stop the consumer using ctrl c, confirm it
#okay whenever we start a consumer it assigns it a new grp id which we can deny
#by providing it existing grp id to do grouping, okay we can do these using commands
#okay to see the number of partition in topic we can use describe topic us it too,

#Segments, Commit Log & Retention Policy
#segments-messages get append in the partition, so the set of message is called segment,
#we can define size of segment, we use this in commit log, actually the message get stored in file system,
#in the config we have these server.properties inside which we have commit log directory,
#where actual messages are stored, okay specifically in the tmp/kafka_logs contains all .log logs,
#so all the things produced by producer we gwt there in the logs,
#we can open this file using the path in our cmd and can see all the data
#Retention policy-- now we are putting all these data in the log files okay now until when these data
#will be there is decided by retention policy 1. size based policy and 2. time based policy
#okay so these all process is done by kafka log cleaner he keeps check on the retention policy of the message and deletes them okay,
#log file is encoded, producer encodes it and put it here, and the consumer decodes it,
#by default retention hours iss 168 hours which is 7 days okay similar default size limit is there

# 3-Broker cluster setup
# okay to start kafka broker we use server start in the bin at cmd, in ubuntu folder and cmd can be different
#okay the first step to create a broker is to copy paste the server,properties and rename as 1,2,3 okay
#step 2--> open both the files and then change the broker id which is unique for both,
#same for the port and logs file, we make unique okay
#okay then in next step we start the server using different server.properties files that we created,
#first make sure that one zookeeper for all the instance that we are creating then only they all
#will be part of the same cluster and will form a kafka broker
#okay use replication factor & partition while creating, okay replication factor means that topic
#will be replicated given number of times, okay when the producer sends the message it goes in the
#form of partition in the topic, and the topic goes to the first broker and then get replicated to the other
#broker as well, for fault tolerance, if that broker goes down then it can pick up the data from the other broker
#number of kafka server cells running is the number of brokers okay so the first topic will be given to the leader
#always okay means let say the broker 1 is leader for partition 3 and the and 2 is for partition 1 then
#if the message goes to whose partition becomes the broker leader and sends the topic copy's to the other brokers
#for the newer kafka version after creating server we give bootstrap server, which is in our case will be a list
#as we are creating a kafka broker so we will pass the three server ports, i guess along with all commands
#we have to pass all our three servers for the cmd we are using form start

#ISR--in sync replica

#managed kafka cloud--confluent cloud
#we not have to make all this dirt on our local okay we can us cloud kafka
#for more we can refer to the video

#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#video-42 | need to look this again too much complex video, need to understand and take a complete day to process this clearly as did with the kafka
#topic-jwt authentication

#jwt-json web token is a way to securely transmit information between parties as json object ,
#jwt is a compact, url safe token that can carry information between parties, it uses base 64 url encoding ,
#it picks such character in url so that no encoding issue,
#jwt is a string consisting 3 parts separated by dots, Header-->payload-->signature HMAC SHA256 or RSA
#payload-payload contains the claim, claims are the statement about an entity,means user metadata
#signature-- search on chatgpt

#jwt configuration--
#we add the 3 dependency one is api other is its implementation, and third is jackson
#to work with json which is an json processing library
#then we will create a package utils and a class named JwtUtil here we will
#write frequently used method, okay then in video he has replaced
#the create username to sign up and duplicate it again to login, now when the users
#login, then in login response we give jwt token to user, then in the next request's user will
#take jwt token along to sign in not the username and password as we did earlier okay
#okay for the login user to get the token we need to check the user which is requesting
#login is valid user the username and password are correct basically we will authenticate the user at login

#then autowire authentication manager using this we will authenticate our user, by passing new UsernamePasswordAuthToken
#okay first make sure that we have configured its bean in the config
#okay so the authentication manager internally checks the username and password by calling both the methods load_by_username and the bcrypt_password method
#then again we use loadByUsername again to get the user details, then we autowire the jwt util okay then we call generate token method on jwt util and pass username in it
#then we return that jwt token

#now we will know all the things return in the jwt util class all the methods okay one by one starting from generate token
#okay now the generate token method take username as argument okay, and then we create one map which in which we can send our user metadata if we want or it can be empty no issues
#okay then we call another method which is create token which takes two arguments one is claims(Map) and second is subject(String)
#in our case username is the subject which we have sent in the method call, then we have called alot of method on Jwts.builder()
#we can check the jwt util for more details, or also we can refer to the jwt documentation, okay in the method chaining we call signWith(getSigningKeu)
#okay so in the getSigningKey method which returns the Keys.hmacShaKeyFor(SECRET_KEY.getBytes()) okay where secret key is a private variable which <= 32 bytes




#--------------------------------------------------------------------------------------------------------------------------------------------------

#video-43
#topic-kafka fallback

#okay here we write the kafka in try catch if kafka is down in some case then we call the service directly
#we have not implemented the kafka as it is paid not for free so we have understood the concepts okay but not created
#the consumer and producer in the code and used the kafka template okay we will do it later
#listen in the whole project we have implemented everything exact till redis cache okay till 40th video whole code is same
#we just have not implemented the kafka in code but understood the concept well, video-41
#okay we have implemented the jwt video-42 in the code but yet to understand it
#then in video 43 it is ofr kafka fallback, which i have described above as we have not implemented it as well but understood okay
#so the summary is that except these 3 videos 41,42,43 we have implemented everything we just need to look into this mainly jwt video 42 rest is good

#-----------------------------------------------------------------------------------------------------------------------------------

#video-44
#topic-deployment on cloud

